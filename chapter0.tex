%----------------------------------------------------------------------------
\chapter{Background}\label{sect:Background}
%----------------------------------------------------------------------------
\section{Summarization}
Summarization is the task of text shortening with the least amount of information loss. Being able to do this automatically and with nearly human level accuracy is a challenge to this date but its usefulness is undeniable especially in the news industry where there is a constant supply of new articles and writing a summary for each of them would be a waste of resources if there would be another way of summarizing them.

We can distinguish between two approaches of this problem, both interesting and challenging.
\rg{ne legyenek egymondatos bekezdések}

\subsection{Abstractive summarizaton}
Summaries written by humans are usually free-form, not a word-by-word or sentence-by-sentence cut outs from the article.

The goal of abstractive summarization is to construct the summaries in a similar fashion, being creative with its word choice so the end summary is as sound and as readable as any summary written by a person.

This poses multiple difficulties because the system not only has to figure out what is important in an article but also has to construct grammatically correct sentences and are at the same time semantically sound and are actually a good summary of the article.

This is a hard compared to the extractive approach but recently there have been some breakthroughs with end-to-end seq2seq neural networks.

\rg{egyrészt hivatkozz 2-3 cikket, egy közülük Abigail See cikke legyen a
Pointer Generator networkökrol, a többi lehet a tavalyi ACLrol. De ami még
fontosabb: le kell írni, hogy ezek mennyire használhatatlanok még, mivel
rendszeresen halandzsáznak és/vagy tartalmilag fals dolgokat írnak le. És ezért
fel nem merül, hogy real-world alkalmazásokban használjuk oket}

\subsection{Extractive summarization}
Extractive summarization is an easier approach to the problem of summarization because it just chooses the expressions or sentences to keep from the original text.

One could think of this as highlighting the most important information in a text.

The mayor advantage of extractive summarization is the fact that there is no need to paraphrase anything from the original text, but the downside is that the maximum achievable ROUGE score is limited by the sentences of the text.

I've decided to choose this approach for my thesis but it's important to mention, that on its own my Graph Neural Network model could be used for abstractive summarisationas well, the pre- and postprocessing of the data is what determines which approach will be used.
\rg{ezt kicsit óvatosabban fogalmazd, új helyes mondatokat generálni UD
részgráfokból az nem hogy nem postprocessing, hanem egy teljesen új és nehéz
feladat lenne. Inkább írd azt, hogy lesznek erre is ötleteid a kitekintés
fejezetben (surface realization, stb}


\section{Evaluation}
\rg{mielott a metrikákról írnál, kell egy kis bevezeto, amibol kiderül, hogy
általában az az elv, hogy emberek által írt summarykkel való overlapot nézünk
evaluáció gyanánt, és hogy ennek eleve mekkora korlátoltsága, hogy egy nagyon
tökéletlen proxy, hiszen egy rendszer írhatna egy szuperjó, de az emberétol jól
eltéro summaryt.}

\subsection{BLEU score}
The BLEU~\cite{BLEU} score was first introduced as an automatic evaluation tool for machine translation in 2002 but it has been used for a multitude of problems on the field of Natural Language Processing.

BLEU is a precision oriented measure

\[BLEU = \frac{\sum_{S \in CandidateSummaries}\sum_{n\_gram \in S} Count_{clip} (n\_grams)}{\sum_{S \in CandidateSummaries}\sum_{n\_gram' \in S} Count(n\_grams')}\]

\subsection{ROUGE score}
The ROUGE score metric has been developed in 2004 by Chin-Yew Lin~\cite{ROUGE}. ROUGE is short for Recall-Oriented Understudy for Gisting Evaluation. The metric is used to determine the quality of a generated summary by comparing it to a set of human written summaries. It yields similar results to human intuition.

\textbf{ROUGE-N} is the ROUGE measure over n-grams. An n-gram is an expression from the text with length n.

In contrast to the BLEU score which is a precision score, the ROUGE-N scores are the recall of the candidate summaries.

\[ROUGE-N = \frac{\sum_{S \in ReferenceSummaries}\sum_{n\_gram \in S} Count_{match} (n\_grams)}{\sum_{S \in ReferenceSummaries}\sum_{n\_gram \in S} Count(n\_grams)}\]

Where the \(Count_{match}(n\_grams)\) is the maximum number of co-occurring n-grams in the summaries.

If n is 1, the ROUGE score is simply the number of matching words divided by the number of words in the reference sentence.

\textbf{ROUGE-L} is an F-measure using the longest common subsequence of the predicted and reference summaries.

\[Recall_{lcs} = \frac{LCS(reference\ summary,\ candidate\ summary)}{|reference\ summary|}\]

\[Precision_{lcs} = \frac{LCS(reference\ summary,\ candidate\ summary)}{|candidate\ summary|}\]

\[ROUGE-L = F1_{lcs} = \frac{(1 + \beta^2)Recall_{lcs}Precision_{lcs}}{Recall_{lcs} + \beta^2Precision_{lcs}}\]

Where LCS is a function returning the length of the longest common subsequence and \(\beta = \frac{Precision_{lcs}}{Recall_{lcs}}\)

\textbf{ROUGE-W} uses the weighted longest common subsequence method instead of the simple \textit{LCS} that prefers consecutive matches so the weighting function scores them higher.

\textbf{ROUGE-S} is a Skip-Bigram Co-Occurrence Statistic. Skip-bigrams are any pair of words in the article, allowing for gaps between them. Skip-bigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate summary and a set of reference summaries.

\[Recall_{skip\_2} = \frac{SKIP_2(X, Y)}{combine(m, 2)}\]
\[Precision_{skip\_2} = \frac{SKIP_2(X, Y)}{combine(k, 2)}\]
\[ROUGE-S = F1_{skip\_2} = \frac{(1 + \beta^2)Recall_{skip\_2}Precision_{skip\_2}}{Recall_{skip\_2} + \beta^2Precision_{skip\_2}}\]

Although the ROUGE-S allows skips in the matches but it is still sensitive to word order.

\textbf{ROUGE-SU} is an expansion over the ROUGE-S metric adding a unigram feature to it to account for single word matches.

\rg{nagyon hiányoznak a hivatkozások, és annak leírása, hogy a ROUGE, ha nem is
explicit azt méri, amit szeretnénk (ld. az elozo kommentemet), de legalább
baromi magasan korrelál a human evaluationnel. Az alap ROUGE cikken kívül
hivatkozandó még a "... shades of ROUGE" c. újabb cikk. És leírandó, hogy a
ROUGE-2 korrelál a legjobban, és  a ROUGE-1-et és -L-t szokták a legtöbbet
használni. + én még leírnám azt is, hogy ROUGE alapon összehasonlítani csak úgy
van értelme rendszereket, ha ugyanolyan hosszú outputokat kértünk tolük, mivel
hogy recall oriented - tehát a hosszabb summary automatikusan magasabb ROUGE-t
érhet el.

\section{Previous work}
The summmarization is a long standing task. There have been multiple attempts at solving this problem.


\subsection{Greedy method}
\rg{állj, itt keveredik két dolog. amit most leírsz, az nem egy summarization
method, hanem arra egy módszer, hogy ha van human summarym, akkor felülrol
becsüljem az extractive módszerrel elérheto ROUGE scoret}


The (naive) greedy method works with a very basic main principle, always choose the sentence, that maximizes the ROUGE score of the summary.


\begin{algorithm}
	\SetAlgoLined
	\KwResult{The n best sentence from the text}
	best\_sentences = ""\;
	\For{i < number of required sentences}{
		rouge\_scores = map()\;
		\ForEach{sentence in text \(\notin\) best\_sentences}{
			rouge\_scores[concat(best\_sentences, sentence)] =  rouge\_score(sentence)\;
		}
		best\_sentence = max\_key\_by\_value(rouge\_scores)\;
		best\_sentences = concat(best\_sentences, best\_sentence)\;
		i = i + 1\;
	}
	\caption{Greedy summarization algorithm}
\end{algorithm}

\rg{és hogy egy konkrét adaton (CNN-DM) mi ez a felso becslés, az majd a
datasetek bemutatásánál legyen, ne itt! Ott leírhatod majd azt is, hogy ki
milyen pontszámokat ért el rajta}

\begin{longtable}{| l | l | l |}
	\hline
	\textbf{Evaluation method}&\textbf{Metric}&\textbf{Score}\\ \hline \hline
		\multirow{3}{*}{\textbf{ROUGE-1}}
			&Average Recall&0.73912\\
			&Average Precision&0.33846 \\
			&Average F1&0.44619 \\ \hline \hline
		\multirow{3}{*}{\textbf{ROUGE-2}}
			&Average Recall&0.39292 \\
			&Average Precision&0.18290 \\
			&Average F1&0.23900 \\ \hline \hline
		\multirow{3}{*}{\textbf{ROUGE-3}}
			&Average Recall&0.24648 \\
			&Average Precision&0.11801 \\
			&Average F1&0.15226 \\ \hline \hline
		\multirow{3}{*}{\textbf{ROUGE-4}}
			&Average Recall&0.17230 \\
			&Average Precision&0.08483 \\
			&Average F1&0.10807 \\ \hline \hline
		\multirow{3}{*}{\textbf{ROUGE-L}}
			&Average Recall&0.51007 \\
			&Average Precision&0.23272 \\
			&Average F1&0.30654 \\ \hline \hline
		\multirow{3}{*}{\textbf{ROUGE-W-1.2}}
			&Average Recall&0.19517 \\
			&Average Precision&0.18583 \\
			&Average F1&0.17739 \\ \hline \hline
		\multirow{3}{*}{\textbf{ROUGE-S*}}
			&Average Recall&0.48162 \\
			&Average Precision&0.11587 \\
			&Average F1&0.16621 \\ \hline \hline
		\multirow{3}{*}{\textbf{ROUGE-SU*}}
			&Average Recall&0.49349 \\
			&Average Precision&0.12013 \\
			&Average F1&0.17242 \\ \hline
		\caption{ROUGE scores of the top 4 sentences chosen by the naive greedy algorithm over the test set of the CNN-Daily Mail data}
		\label{tab:extr}
\end{longtable}
As the table~\ref{tab:extr} shows this method works surprisingly well.

There have been some advancement of this method regarding its speed\cite{GreedySum}.

\subsection{Text Rank}

The Text Rank algorithm is an unsupervised method for automatic text summarization and it can also be used for keyword extraction. It has first described in a paper titled TextRank: Bringing Order into Texts~\cite{TextRank:2004}.

TextRank is a graph based method that (for summarization purposes) builds graph representations from the articles using similarity measures of the sentences in them. Then the algorithm scores each of the vertices is calculated with the following formula:

\[score(V_i) = (1 - d) + d \sum_{j \in InComingTo(V_i)} \frac{1}{|OutGoingFrom(V_j)|} score(V_j)\]

where d is a damping factor between 0 and 1.

The article~\cite{TextRank} also referenced by the \href{https://radimrehurek.com/gensim/summarization/summariser.html}{gensim documentation} describes multiple alternative methods for Text Rank. Gensim uses one of these alternative methods for summary extraction.  

\begin{longtable}{| l | l | l |}
	\hline
	\textbf{Evaluation method}&\textbf{Metric}&\textbf{Score}\\ \hline \hline
	\multirow{3}{*}{\textbf{ROUGE-1}}
		&Average Recall&0.56350 \\
		&Average Precision&0.20366  \\
		&Average F1&0.28692  \\ \hline \hline
	\multirow{3}{*}{\textbf{ROUGE-2}}
		&Average Recall&0.20801 \\
		&Average Precision&0.07546 \\
		&Average F1&0.10587 \\ \hline \hline
	\multirow{3}{*}{\textbf{ROUGE-3}}
		&Average Recall&0.11009 \\
		&Average Precision&0.04068 \\
		&Average F1&0.05656 \\ \hline \hline
	\multirow{3}{*}{\textbf{ROUGE-4}}
		&Average Recall&0.07074 \\
		&Average Precision&0.02666 \\
		&Average F1&0.03671 \\ \hline \hline
	\multirow{3}{*}{\textbf{ROUGE-L}}
		&Average Recall&0.34467 \\
		&Average Precision&0.12093 \\
		&Average F1&0.17148 \\ \hline \hline
	\multirow{3}{*}{\textbf{ROUGE-W-1.2}}
		&Average Recall&0.12907 \\
		&Average Precision&0.09360 \\
		&Average F1&0.10068 \\ \hline \hline
	\multirow{3}{*}{\textbf{ROUGE-S*}}
		&Average Recall&0.27925 \\
		&Average Precision&0.04148 \\
		&Average F1&0.06513 \\ \hline \hline
	\multirow{3}{*}{\textbf{ROUGE-SU*}}
		&Average Recall&0.29271 \\
		&Average Precision&0.04401 \\
		&Average F1&0.06914 \\ \hline
	\caption{ROUGE scores of the top 4 sentences chosen by the gensim library's TextRank variation based summarization algorithm over the test set of the CNN-Daily Mail data}
\end{longtable}

I will use this as a benchmark for my model evaluation.

\subsection{Deep Learning models}

The highest achieving models on this task are end-to-end deep learning models, mostly ones using some kind of pretrained language models.
The best model currently is Text Summarization with Pretrained Encoders~\cite{BERTsum}. This model has both extractive and abstractive versions.

\begin{longtable}{| l | l |}
	\hline 
	\textbf{Metric}&\textbf{Score} \\ \hline \hline
	\textbf{ROUGE-1 F1}&0.4385 \\ \hline
	\textbf{ROUGE-2 F1}&0.2034 \\ \hline
	\textbf{ROUGE-L F1}&0.3990 \\ \hline
	\caption{Their best results on the CNN-Daily Mail dataset according to the paper}
\end{longtable}

The second best based only on the ROUGE-1 score is the paper titled Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer~\cite{TransferSum} which works with the popular transformer approach.

\begin{longtable}{| l | l |}
	\hline 
	\textbf{Metric}&\textbf{Score} \\ \hline \hline
	\textbf{ROUGE-1 F1}&0.4352 \\ \hline
	\textbf{ROUGE-2 F1}&0.2155 \\ \hline
	\textbf{ROUGE-L F1}&0.4069 \\ \hline
	\caption{Their best results on the CNN-Daily Mail dataset by their own account}
\end{longtable}
